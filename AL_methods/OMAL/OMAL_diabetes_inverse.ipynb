{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0355f92-7104-4030-8e8c-2aebd9837375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from skorch.regressor import NeuralNetRegressor\n",
    "from skorch.dataset import ValidSplit\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, ReLU\n",
    "from torch.nn import Module\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from torch import tensor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from model import MLP\n",
    "from model_state import get_model_params, get_input_for_hidden_layers, get_model_params_gradientNorm\n",
    "from functions_batch1 import get_variance, qbc, normalization, error_reduct_fuc, total_disagrement, grad_norm, averaged_grad_norm, get_avg_grad_norm, training_loss\n",
    "import random\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Freezer, Unfreezer\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from skorch.regressor import NeuralNetRegressor, NeuralNet\n",
    "from model_state import features_concat, learning_state_features_concat\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dceaa95-f840-4923-9958-b246f5d1e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params:\n",
    "query_number = 25          # The number of the AL Iterations in each Exp\n",
    "iteration = 20             # Total number of the Exps\n",
    "batch_size = 10\n",
    "total_initail_size = 11\n",
    "initial_size = 5\n",
    "dimensions = 18   # Total dimension of the data features + model states\n",
    "\n",
    "seed_rf = np.load(file=\"..\\..\\Seeds\\seed2.npy\")\n",
    "seed_initial = np.load(file=\"..\\..\\Seeds\\seed3.npy\")\n",
    "\n",
    "seed_nn1 = np.load(file=\"..\\..\\Seeds\\seed4.npy\")\n",
    "seed_nn2 = np.load(file=\"..\\..\\Seeds\\seed4.npy\")\n",
    "seed_nn3 = np.load(file=\"..\\..\\Seeds\\seed5.npy\")\n",
    "seed_nn4 = np.load(file=\"..\\..\\Seeds\\seed5.npy\")\n",
    "seed_nn5 = np.load(file=\"..\\..\\Seeds\\seed6.npy\")\n",
    "seed_nn6 = np.load(file=\"..\\..\\Seeds\\seed6.npy\")\n",
    "\n",
    "seed_predictor = np.load(file=\"..\\..\\Seeds\\seed7.npy\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3664799-9b42-448e-94de-f424b15c7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Committee_Prediction(learner_list, X_test, y_test):\n",
    "    initial_pred=[]\n",
    "    for i in learner_list:\n",
    "        initial_pred.append(r2_score(y_test, i.predict(X_test)))\n",
    "    initial_pred=np.array(initial_pred)\n",
    "    \n",
    "    return initial_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99ffa3-19a4-4bb9-87e5-b6fa04fab8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_argmax(values, n_instances):\n",
    "\n",
    "    assert n_instances <= values.shape[0], 'n_instances must be less or equal than the size of utility.'\n",
    "\n",
    "    max_idx = np.argpartition(-values, n_instances-1, axis=0)[:n_instances]\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488cd6c-903f-4969-88eb-404a307cfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(query_number, iters):\n",
    "    # For saving results:\n",
    "    rf_model_training_r2 = []\n",
    "    rf_model_training_mse = []\n",
    "    rf_model_testing_r2 = []\n",
    "    rf_model_testing_mse = []\n",
    "    \n",
    "    # Number of member in committee:\n",
    "    n_members = 3\n",
    "    learner_list = []\n",
    "    \n",
    "    # The model for evaluation:\n",
    "    rf_model = RandomForestRegressor(random_state=seed_rf[iters], n_estimators=100)\n",
    "    gradient_norms = np.empty(shape=(0, 0))\n",
    "    \n",
    "    # Load the data:\n",
    "    name1 = \"..\\..\\Datasets\\Diabetes\\X_train\" + str(iters) + \".npy\"\n",
    "    name2 = \"..\\..\\Datasets\\Diabetes\\X_test\" + str(iters) + \".npy\"\n",
    "    name3 = \"..\\..\\Datasets\\Diabetes\\y_train\" + str(iters) + \".npy\"\n",
    "    name4 = \"..\\..\\Datasets\\Diabetes\\y_test\" + str(iters) + \".npy\"\n",
    "    \n",
    "    X_train = np.load(name1, allow_pickle=True).astype(np.float32)\n",
    "    X_test = np.load(name2, allow_pickle=True).astype(np.float32)\n",
    "    y_train = np.load(name3, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    y_test = np.load(name4, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Dimensionality of data:\n",
    "    X = X_train.shape[1]\n",
    "    # The index of the unlabeled pool:\n",
    "    X_index = np.arange(X_train.shape[0])\n",
    "\n",
    "    used_data = np.empty(shape=(0, X))\n",
    "    used_label = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    X_initial = np.empty(shape=(0,X))\n",
    "    y_initial = np.empty(shape=(0)).reshape(-1, 1)\n",
    "    \n",
    "    # Initial Stage 1:\n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "\n",
    "    X_initial = X_train[train_idx]\n",
    "    y_initial = y_train[train_idx].reshape(-1, 1)\n",
    "\n",
    "    used_data = np.append(used_data, X_initial, axis=0).astype(np.float32)\n",
    "    used_label = np.append(used_label, y_initial, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "    used_data = torch.from_numpy(used_data).to(device)\n",
    "    used_label = torch.from_numpy(used_label).to(device)\n",
    "    for member_idx in range(n_members):\n",
    "        if member_idx == 0:\n",
    "            np.random.seed(seed_nn1[iters])\n",
    "            torch.manual_seed(seed_nn2[iters])\n",
    "        if member_idx == 1:\n",
    "            np.random.seed(seed_nn3[iters])\n",
    "            torch.manual_seed(seed_nn4[iters])\n",
    "        if member_idx == 2:\n",
    "            np.random.seed(seed_nn5[iters])\n",
    "            torch.manual_seed(seed_nn6[iters])\n",
    "            \n",
    "        regressor = NeuralNetRegressor(MLP(X),\n",
    "                                   criterion=MSELoss(),\n",
    "                                   optimizer=torch.optim.Adam,\n",
    "                                   verbose=0,\n",
    "                                   max_epochs=30,\n",
    "                                   lr=0.001,\n",
    "                                   # Used for the batch AL\n",
    "                                   callbacks=[EarlyStopping(patience=5), ('lr_scheduler', LRScheduler(policy=ReduceLROnPlateau))],\n",
    "                                   train_split=ValidSplit(cv=5),\n",
    "                                   warm_start=True,\n",
    "                                   device='cuda',\n",
    "                                   batch_size = 200\n",
    "                                   )\n",
    "        regressor.fit(used_data, used_label)\n",
    "        learner_list.append(regressor)\n",
    "    \n",
    "    print(\"NN Test R2 with 5 samples\", Committee_Prediction(learner_list, X_test, y_test))\n",
    "\n",
    "    # Number of LAL features:\n",
    "    learning_state_features = np.empty(shape=(0, dimensions))\n",
    "    loss_reduction_target = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    # other random samples:\n",
    "    rest_initial_X = np.empty(shape=(0, X))\n",
    "    rest_initial_y = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    # LAL Initialization: Stage 2\n",
    "    predictor = RandomForestRegressor(n_estimators=1000, random_state=seed_predictor[iters])\n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=total_initail_size-initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "    \n",
    "    rest_initial_X = np.append(rest_initial_X, X_train[train_idx], axis=0).astype(np.float32)\n",
    "    rest_initial_y = np.append(rest_initial_y, y_train[train_idx], axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "    for i in range(rest_initial_X.shape[0]):\n",
    "\n",
    "        single_X = rest_initial_X[i],\n",
    "        single_y = rest_initial_y[i].reshape(1, -1)\n",
    "        single_X = single_X[0].reshape(1, -1)\n",
    "        \n",
    "        single_X = torch.from_numpy(single_X).to(device)\n",
    "        single_y = torch.from_numpy(single_y).to(device)\n",
    "\n",
    "        # Model params\n",
    "        model_params = get_model_params_gradientNorm(learner_list)\n",
    "\n",
    "        # updated data\n",
    "        used_data = torch.cat((used_data, single_X), 0)\n",
    "        used_label = torch.cat((used_label, single_y), 0)\n",
    "\n",
    "        # Retrain the model:\n",
    "        for l in learner_list:\n",
    "            l.fit(X=used_data,y=used_label)\n",
    "            \n",
    "        single_X = single_X.cpu().numpy()\n",
    "        single_y = single_y.cpu().numpy()\n",
    "            \n",
    "        index = train_idx[i]\n",
    "\n",
    "        # The training sample:\n",
    "        data_params = X_train[index]\n",
    "      \n",
    "        LALfeatures = features_concat(model_params, data_params)\n",
    "\n",
    "        loss_reduction = get_avg_grad_norm(learner_list, single_X, single_y)\n",
    "        \n",
    "        learning_state_features = learning_state_features_concat(learning_state_features, LALfeatures)\n",
    "        loss_reduction_target = np.append(loss_reduction_target, np.array([loss_reduction]).reshape(-1, 1), axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "        \n",
    "    used_data = used_data.cpu().numpy()\n",
    "    used_label = used_label.cpu().numpy()\n",
    "\n",
    "    # Finished the Initialization Stages:\n",
    "    # RF Regressor for evaluation:\n",
    "    rf_model.fit(used_data, used_label.ravel())\n",
    "    # Training Scores:\n",
    "    rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "    rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "    rf_model_training_r2.append(rf_training_r2)\n",
    "    rf_model_training_mse.append(rf_training_mse)\n",
    "    \n",
    "    # Test Scores:\n",
    "    rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "    rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "    rf_model_testing_r2.append(rf_model_r2)\n",
    "    rf_model_testing_mse.append(rf_model_mse)\n",
    "    \n",
    "    print(\"After Initialization RF R2:\", rf_model_r2)\n",
    "    print(\"NN Test R2 after Initialization\", Committee_Prediction(learner_list, X_test, y_test))\n",
    "\n",
    "    print(np.unique(used_data, axis=0).shape)\n",
    "    for idx in range(query_number):\n",
    "        np.random.seed(None)\n",
    "        print('Query no. %d' % (idx+1))\n",
    "\n",
    "        Error_reduction_list, training_score = error_reduct_fuc(X_train, X_index, learner_list, learning_state_features, loss_reduction_target, X_index.shape[0], predictor)\n",
    "\n",
    "        # idx = multi_argmax(np.array(Error_reduction_list), n_instances=batch_size)\n",
    "        # Inversed Experiments:\n",
    "        if any(x < 0 for x in Error_reduction_list):\n",
    "            print(\"The NumPy Array has at least one negative value\")\n",
    "        idx = multi_argmax(-1*np.array(Error_reduction_list), n_instances=batch_size)\n",
    "        ####### \n",
    "        \n",
    "        X_train_indices = X_index[idx]\n",
    "        \n",
    "        # Update\n",
    "        X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "        for X_train_index in X_train_indices:\n",
    "\n",
    "            new_X = X_train[X_train_index].reshape(1, -1)\n",
    "            new_y = y_train[X_train_index].reshape(1, -1)\n",
    "            \n",
    "            used_data = np.append(used_data, new_X, axis=0).astype(np.float32)\n",
    "            used_label = np.append(used_label, new_y, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "            used_data = torch.from_numpy(used_data).to(device)\n",
    "            used_label = torch.from_numpy(used_label).to(device)\n",
    "\n",
    "            # Model params\n",
    "            model_params = get_model_params_gradientNorm(learner_list)\n",
    "            data_params = X_train[X_train_index]\n",
    "            LALfeatures = features_concat(model_params, data_params)\n",
    "            \n",
    "            for l in learner_list:\n",
    "                l.fit(used_data, used_label)\n",
    "            \n",
    "            used_data = used_data.cpu().numpy()\n",
    "            used_label = used_label.cpu().numpy()\n",
    "\n",
    "            loss_reduction = get_avg_grad_norm(learner_list, new_X, new_y)  \n",
    "            gradient_norms = np.append(gradient_norms, loss_reduction)\n",
    "            \n",
    "            learning_state_features = learning_state_features_concat(learning_state_features, LALfeatures)\n",
    "            loss_reduction_target = np.append(loss_reduction_target, np.array([loss_reduction]).reshape(-1, 1), axis=0).astype(np.float32).reshape(-1, 1)\n",
    "        \n",
    "\n",
    "        # RF Evaluation\n",
    "        rf_model.fit(used_data, used_label.ravel())\n",
    "        \n",
    "        # Training Evaluation:\n",
    "        rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "        rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "        rf_model_training_r2.append(rf_training_r2)\n",
    "        rf_model_training_mse.append(rf_training_mse)\n",
    "        \n",
    "        # Test Evaluation:\n",
    "        rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "        rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "        rf_model_testing_r2.append(rf_model_r2)\n",
    "        rf_model_testing_mse.append(rf_model_mse)\n",
    "\n",
    "        print(np.unique(used_data, axis=0).shape)\n",
    "        print(\"RF R2:\", rf_model_r2)\n",
    "        print(\"Remaining:\", X_index.shape[0])\n",
    "        # print('NN R2', Committee_Prediction(learner_list, X_test, y_test))\n",
    "\n",
    "    # RF\n",
    "    rf_model_testing_r2 = np.array(rf_model_testing_r2)\n",
    "    rf_model_testing_mse = np.array(rf_model_testing_mse)\n",
    "    rf_model_training_r2 = np.array(rf_model_training_r2)\n",
    "    rf_model_training_mse = np.array(rf_model_training_mse)\n",
    "    \n",
    "    # Used_data and Unsed_label:\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\OMAL_inverse\\Summary\\\\used_data\" + str(iters) + \".npy\", arr=used_data)\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\OMAL_inverse\\Summary\\\\used_labels\" + str(iters) + \".npy\", arr=used_label)\n",
    "\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\OMAL_inverse\\Summary\\\\testing_rf_r2_\" + str(iters) + \".npy\", arr=rf_model_testing_r2)\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\OMAL_inverse\\Summary\\\\testing_rf_mse_\" + str(iters) + \".npy\", arr=rf_model_testing_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3f10b-dc78-4f89-97cb-8e635c25e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iteration):\n",
    "    print(\"The Iteration is \", i)\n",
    "    main_function(query_number, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
