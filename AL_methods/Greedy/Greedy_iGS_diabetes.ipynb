{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc6b04-ff91-4ad6-80a7-daae59ddd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from skorch.regressor import NeuralNetRegressor\n",
    "from copy import deepcopy\n",
    "from skorch.dataset import ValidSplit\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, ReLU\n",
    "from torch.nn import Module\n",
    "from sklearn.metrics import r2_score\n",
    "import copy\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from torch import tensor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from model import MLP\n",
    "import random\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Freezer, Unfreezer\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from skorch.regressor import NeuralNetRegressor, NeuralNet\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7a198-c3dd-4a67-8c4a-ebe0f17d6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params:\n",
    "query_number = 25          # The number of the AL Iterations in each Exp\n",
    "iteration = 20             # Total number of the Exps\n",
    "batch_size = 10\n",
    "total_initail_size = 11\n",
    "initial_size = 5\n",
    "\n",
    "seed_rf = np.load(file=\"..\\..\\Seeds\\seed2.npy\")\n",
    "seed_initial = np.load(file=\"..\\..\\Seeds\\seed3.npy\")\n",
    "\n",
    "seed_nn1 = np.load(file=\"..\\..\\Seeds\\seed4.npy\")\n",
    "seed_nn2 = np.load(file=\"..\\..\\Seeds\\seed4.npy\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7381b5e-ab07-47d9-9418-5ad7e9bc0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_argmax(values, n_instances):\n",
    "\n",
    "    assert n_instances <= values.shape[0], 'n_instances must be less or equal than the size of utility.'\n",
    "\n",
    "    max_idx = np.argpartition(-values, n_instances-1, axis=0)[:n_instances]\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bcf3a-ce67-4a01-acdd-e4998b5672bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_dist(unlabeled_data, labeled_data):\n",
    "\n",
    "    # Should be global info instead of within the clusters\n",
    "    d = cdist(unlabeled_data, labeled_data, 'euclidean')\n",
    "    d_x_min = d.min(axis=1)\n",
    "\n",
    "    return d_x_min\n",
    "\n",
    "def get_closest_dist_xy(unlabeled_predictions, labeled_data_label, unlabeled_data, labeled_data):\n",
    "\n",
    "    d_y = cdist(np.array(unlabeled_predictions).reshape(-1, 1), labeled_data_label, 'euclidean')\n",
    "    d_x = cdist(unlabeled_data, labeled_data, 'euclidean')\n",
    "    d_xy = np.multiply(d_x, d_y)\n",
    "    d_xy_min = d_xy.min(axis=1)\n",
    "\n",
    "    return d_xy_min\n",
    "\n",
    "def custom_query_strategy(used_data, X_pool, prediction_, used_label, n_instances):\n",
    "    d_xy = get_closest_dist_xy(prediction_, used_label, X_pool, used_data)\n",
    "    utility = d_xy\n",
    "    return multi_argmax(utility, n_instances=n_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8e86e-f470-4948-aeed-27e1de4f4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(query_number, iters):\n",
    "    # For saving results:\n",
    "    rf_model_training_r2 = []\n",
    "    rf_model_training_mse = []\n",
    "    rf_model_testing_r2 = []\n",
    "    rf_model_testing_mse = []\n",
    "    \n",
    "    # The model for evaluation:\n",
    "    rf_model = RandomForestRegressor(random_state=seed_rf[iters], n_estimators=100)\n",
    "\n",
    "    # Load the data:\n",
    "    name1 = \"..\\..\\Datasets\\Diabetes\\X_train\" + str(iters) + \".npy\"\n",
    "    name2 = \"..\\..\\Datasets\\Diabetes\\X_test\" + str(iters) + \".npy\"\n",
    "    name3 = \"..\\..\\Datasets\\Diabetes\\y_train\" + str(iters) + \".npy\"\n",
    "    name4 = \"..\\..\\Datasets\\Diabetes\\y_test\" + str(iters) + \".npy\"\n",
    "    \n",
    "    X_train = np.load(name1, allow_pickle=True).astype(np.float32)\n",
    "    X_test = np.load(name2, allow_pickle=True).astype(np.float32)\n",
    "    y_train = np.load(name3, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    y_test = np.load(name4, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Feature Dimï¼›\n",
    "    X = X_train.shape[1]\n",
    "    # The unlabeled pool\n",
    "    X_index = np.arange(X_train.shape[0])\n",
    "    \n",
    "    # Queried samples and labels\n",
    "    used_data = np.empty(shape=(0, X))\n",
    "    used_label = np.empty(shape=(0)).reshape(-1, 1)\n",
    "    \n",
    "    # Initial samples and labels\n",
    "    X_initial = np.empty(shape=(0,X))\n",
    "    y_initial = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    # Initial Stage 1:\n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "\n",
    "    X_initial = X_train[train_idx]\n",
    "    y_initial = y_train[train_idx].reshape(-1, 1)\n",
    "\n",
    "    used_data = np.append(used_data, X_initial, axis=0).astype(np.float32)\n",
    "    used_label = np.append(used_label, y_initial, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "    \n",
    "    # Initial Sagte 2:\n",
    "    rest_initial_X = np.empty(shape=(0,X))\n",
    "    rest_initial_y = np.empty(shape=(0)).reshape(-1, 1)\n",
    "    \n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=total_initail_size-initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "    \n",
    "    rest_initial_X = np.append(rest_initial_X, X_train[train_idx], axis=0).astype(np.float32)\n",
    "    rest_initial_y = np.append(rest_initial_y, y_train[train_idx], axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "    \n",
    "    used_data = np.append(used_data, rest_initial_X, axis=0).astype(np.float32)\n",
    "    used_label = np.append(used_label, rest_initial_y, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Finished the Initialization Stages:\n",
    "    # RF Regressor for evaluation:\n",
    "    rf_model.fit(used_data, used_label.ravel())\n",
    "    # Training Scores:\n",
    "    rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "    rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "    rf_model_training_r2.append(rf_training_r2)\n",
    "    rf_model_training_mse.append(rf_training_mse)\n",
    "    \n",
    "    # Test Scores:\n",
    "    rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "    rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "    rf_model_testing_r2.append(rf_model_r2)\n",
    "    rf_model_testing_mse.append(rf_model_mse)\n",
    "    \n",
    "    np.random.seed(seed_nn1[iters])\n",
    "    torch.manual_seed(seed_nn2[iters])\n",
    "    predictor = NeuralNetRegressor(MLP(X),\n",
    "                                   criterion=MSELoss(),\n",
    "                                   optimizer=torch.optim.Adam,\n",
    "                                   verbose=0,\n",
    "                                   max_epochs=100,\n",
    "                                   lr=0.001,\n",
    "                                   # Used for the batch AL\n",
    "                                   callbacks=[EarlyStopping(patience=20), ('lr_scheduler', LRScheduler(policy=ReduceLROnPlateau))],\n",
    "                                   train_split=ValidSplit(cv=5),\n",
    "                                   warm_start=False,\n",
    "                                   device='cuda',\n",
    "                                   batch_size = 200\n",
    "                                   )\n",
    "    used_data = torch.from_numpy(used_data).to(device)\n",
    "    used_label = torch.from_numpy(used_label).to(device)\n",
    "    # Train:\n",
    "    predictor.fit(used_data, used_label)\n",
    "    print(\"NN Initialization Scores:\", r2_score(y_test, predictor.predict(X_test)))\n",
    "    \n",
    "    print(\"After Initialization RF R2:\", rf_model_r2)\n",
    "\n",
    "    for idx in range(query_number):\n",
    "        np.random.seed(None)\n",
    "        \n",
    "        print('Query no. %d' % (idx+1))\n",
    "        \n",
    "        used_data = used_data.cpu().numpy()\n",
    "        used_label = used_label.cpu().numpy()\n",
    "        predictions = predictor.predict(torch.from_numpy(X_train[X_index]).to(device))\n",
    "        idx = custom_query_strategy(used_data, X_train[X_index], predictions, used_label, batch_size)\n",
    "\n",
    "        # Query the new sample:\n",
    "        X_train_index = X_index[idx]\n",
    "\n",
    "        new_X = X_train[X_train_index].reshape(batch_size, -1)\n",
    "        new_y = y_train[X_train_index].reshape(batch_size, -1)\n",
    "\n",
    "        # Adding the used data to the used_data pool\n",
    "        \n",
    "        used_data = np.append(used_data, new_X, axis=0).astype(np.float32)\n",
    "        used_label = np.append(used_label, new_y, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "        # RF Evaluation\n",
    "        rf_model.fit(used_data, used_label.ravel())\n",
    "        \n",
    "        # Training Evaluation:\n",
    "        rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "        rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "        rf_model_training_r2.append(rf_training_r2)\n",
    "        rf_model_training_mse.append(rf_training_mse)\n",
    "        \n",
    "        # Test Evaluation:\n",
    "        rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "        rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "        rf_model_testing_r2.append(rf_model_r2)\n",
    "        rf_model_testing_mse.append(rf_model_mse)\n",
    "\n",
    "        # remove queried instance from pool\n",
    "        X_index = np.delete(X_index, idx, axis=0)\n",
    "        \n",
    "        print(np.unique(used_data, axis=0).shape)\n",
    "        print(\"RF R2:\", rf_model_r2)\n",
    "        print(\"Remaining:\", X_index.shape[0])\n",
    "        \n",
    "        used_data = torch.from_numpy(used_data).to(device)\n",
    "        used_label = torch.from_numpy(used_label).to(device)\n",
    "        predictor.fit(used_data, used_label)\n",
    "\n",
    "\n",
    "    # RF\n",
    "    rf_model_testing_r2 = np.array(rf_model_testing_r2)\n",
    "    rf_model_testing_mse = np.array(rf_model_testing_mse)\n",
    "    rf_model_training_r2 = np.array(rf_model_training_r2)\n",
    "    rf_model_training_mse = np.array(rf_model_training_mse)\n",
    "    \n",
    "    used_data = used_data.cpu().numpy()\n",
    "    used_label = used_label.cpu().numpy()\n",
    "    # Used_data and Unsed_label:\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\Greedy\\Summary\\\\used_data\" + str(iters) + \".npy\", arr=used_data)\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\Greedy\\Summary\\\\used_labels\" + str(iters) + \".npy\", arr=used_label)\n",
    "\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\Greedy\\Summary\\\\testing_rf_r2_\" + str(iters) + \".npy\", arr=rf_model_testing_r2)\n",
    "    np.save(file=\"..\\..\\Results\\Res_Diabetes\\Greedy\\Summary\\\\testing_rf_mse_\" + str(iters) + \".npy\", arr=rf_model_testing_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343e876-ce01-49de-a4aa-9bb8a7811be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iteration):\n",
    "    print(\"The Iteration is \", i)\n",
    "    main_function(query_number, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
